{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import SGD\n",
    "class neural_network(nn.Module):\n",
    "    def __init__(self, layers=[2, 100, 2], scale=.1, p=None, lr=None, lam=None):\n",
    "        super().__init__()\n",
    "        self.weights = nn.ParameterList([nn.Parameter(scale * tr.randn(m, n)) for m, n in zip(layers[:-1], layers[1:])])\n",
    "        self.biases = nn.ParameterList([nn.Parameter(scale * tr.randn(n)) for n in layers[1:]])\n",
    "        # self.weights = None\n",
    "        # self.biases = None\n",
    "        self.p = p\n",
    "        self.lr = lr\n",
    "        self.lam = lam\n",
    "        self.train_mode = False\n",
    "\n",
    "    def relu(self, X, W, b):\n",
    "        return F.relu(X @ W + b)\n",
    "\n",
    "    def softmax(self, X, W, b):\n",
    "        return F.softmax(X @ W + b, dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = tr.tensor(X, dtype=tr.float)\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            X = self.relu(X, self.weights[i], self.biases[i])\n",
    "            if self.p is not None and self.train_mode:\n",
    "                X = F.dropout(X, p=self.p)\n",
    "\n",
    "        X = self.softmax(X, self.weights[-1], self.biases[-1])\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X).detach().numpy()\n",
    "\n",
    "    def loss(self, ypred, ytrue):\n",
    "        ypred = tr.clamp(ypred, 1e-9, 1 - 1e-9)  # To avoid log(0)\n",
    "        return -tr.mean(tr.sum(ytrue * tr.log(ypred), dim=1))\n",
    "\n",
    "    def fit(self, X, y, nsteps=1000, bs=100, plot=False):\n",
    "        X, y = tr.tensor(X, dtype=tr.float), tr.tensor(y, dtype=tr.float)\n",
    "        optimizer = SGD(self.parameters(), lr=self.lr, weight_decay=self.lam)\n",
    "\n",
    "        I = tr.randperm(X.shape[0])\n",
    "        n = int(np.floor(.9 * X.shape[0]))\n",
    "        Xtrain, ytrain = X[I[:n]], y[I[:n]]\n",
    "        Xval, yval = X[I[n:]], y[I[n:]]\n",
    "\n",
    "        Ltrain, Lval, Aval = [], [], []\n",
    "        for i in range(nsteps):\n",
    "            optimizer.zero_grad()\n",
    "            I = tr.randperm(Xtrain.shape[0])[:bs]\n",
    "            self.train_mode = True\n",
    "            output = self.loss(self.forward(Xtrain[I]), ytrain[I])\n",
    "            self.train_mode = False\n",
    "            Ltrain += [output.item()]\n",
    "            output.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            outval = self.forward(Xval)\n",
    "            Lval += [self.loss(outval, yval).item()]\n",
    "            Aval += [np.array(outval.argmax(-1) == yval.argmax(-1)).mean()]\n",
    "\n",
    "        if plot:\n",
    "            plt.plot(range(nsteps), Ltrain, label='Training loss')\n",
    "            plt.plot(range(nsteps), Lval, label='Validation loss')\n",
    "            plt.plot(range(nsteps), Aval, label='Validation acc')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "# # Example for testing the neural_network class\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create dummy data for testing\n",
    "#     X = np.random.randn(200, 2)\n",
    "#     y = np.eye(2)[(X[:, 0] + X[:, 1] > 0).astype(int)]  # Simple linearly separable data\n",
    "\n",
    "#     # Initialize and train the neural network\n",
    "#     nn = neural_network(layers=[2, 100, 2], scale=.1, lr=0.01, lam=0.01, p=0.5)\n",
    "#     nn.fit(X, y, nsteps=1000, bs=20, plot=True)\n",
    "\n",
    "#     # Predict on new data\n",
    "#     X_test = np.random.randn(50, 2)\n",
    "#     y_pred = nn.predict(X_test)\n",
    "#     print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tue\\AppData\\Local\\Temp\\ipykernel_26276\\1711889869.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X, y = tr.tensor(X, dtype=tr.float), tr.tensor(y, dtype=tr.float)\n",
      "C:\\Users\\Tue\\AppData\\Local\\Temp\\ipykernel_26276\\1711889869.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = tr.tensor(X, dtype=tr.float)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "def test_neural_network(self):\n",
    "        X = tr.tensor([[1, 1], [0, 0]], dtype=tr.float)\n",
    "        y = tr.tensor([[0, 1], [1, 0]], dtype=tr.int)\n",
    "        W = tr.tensor([[1, .2], [.5, 1]], dtype=tr.float)\n",
    "        b = tr.tensor([-1, -1], dtype=tr.float)\n",
    "        m = neural_network(layers=[2,2,2], p=0, lam=0, lr=.1)\n",
    "\n",
    "        m.fit(X, y, nsteps=1, bs=1, plot=False)\n",
    "\n",
    "        relu_out = tr.tensor([[.5,.2],[0, 0]])\n",
    "        # self.assertTrue(np.allclose(m.relu(X, W, b), relu_out), msg='neural_network: Error. ReLU output not correct')\n",
    "\n",
    "        softmax_out = np.array([[0.57444252, 0.42555748], [.5, .5]])\n",
    "        # self.assertTrue(np.allclose(m.softmax(relu_out, W, b), softmax_out), msg='neural_network: Error. Softmax output not correct')\n",
    "\n",
    "        m.weights = tr.nn.ParameterList([tr.nn.Parameter(W), tr.nn.Parameter(W)])\n",
    "        m.biases = tr.nn.ParameterList([tr.nn.Parameter(b), tr.nn.Parameter(b)])\n",
    "        loss_out = 0.7737512125142362\n",
    "        out = m.forward(X)\n",
    "        loss = m.loss(out, y).item()\n",
    "        # self.assertTrue(np.isclose(loss_out, loss), msg='neural_network: Error. Loss output not correct')\n",
    "        # self.assertTrue(np.allclose(out.detach().numpy(), softmax_out), msg='neural_network: Error. Network output not correct')\n",
    "        \n",
    "test_neural_network(unittest.TestCase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
